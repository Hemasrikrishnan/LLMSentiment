{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc733d0-0e64-445c-9641-1443cd96db2b",
   "metadata": {},
   "source": [
    "## AI-driven Customer Feedback Analysis and Chatbot Development for Enhancing User Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9fe3ff-446b-4211-80be-c305eb3de714",
   "metadata": {},
   "source": [
    "Video Link: https://drive.google.com/file/d/18SW-8XKknv-icJ4j6ZRmK2O5H8D3dQ-R/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a571f9-0792-4801-a678-2f4391a7322e",
   "metadata": {},
   "source": [
    "### SCRAPE GOOGLE REVIEW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c26c11a-1902-476c-8301-44064f91e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is collected from google play store for “skiptheDishes” app using google play scraper. \n",
    "from google_play_scraper import app, reviews\n",
    "\n",
    "# Specify the package name of the app for which you want to extract reviews\n",
    "package_name = 'com.ncconsulting.skipthedishes_android'\n",
    "\n",
    "# Fetch the reviews for the specified app\n",
    "result = reviews(package_name, lang='en', count=5000,country=\"ca\",continuation_token=None)  # Specify the number of reviews to fetch\n",
    "\n",
    "# Print the fetched reviews\n",
    "#print(len(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f28523-7039-4661-abcd-68ee62b5a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "reviews_skip=pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbead1e-e94d-4a75-90a0-a8c182b511d8",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0712dd-1535-4160-9094-653a1b839035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hemak\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1592/1592 [08:09<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis's main goal is to learn more about how consumers feel about the SkipTheDishes app in general. \n",
    "#Stakeholders can determine the app's features and services' strong and weak points by analyzing the sentiment distribution among reviews.\n",
    "#We are categorizing the reviews as sentiment score and sentiment label, indicating the class POSITIVE or NEGATIVE, using hugging face transformers \n",
    "#pretrained models. \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "summarization_pipeline = pipeline(\"sentiment-analysis\",model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "reviews_skip['Sentiment_score']=np.nan\n",
    "reviews_skip['Sentiment_class']=np.nan\n",
    "from tqdm import tqdm\n",
    "for j in tqdm(range(0,len(reviews_skip))):\n",
    "    res=summarization_pipeline(reviews_skip['content'][j])\n",
    "    reviews_skip['Sentiment_score'][j]=res[0]['score']\n",
    "    reviews_skip['Sentiment_class'][j]=res[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f68621-d6c5-4d21-85dc-6c46c7b549e0",
   "metadata": {},
   "source": [
    "### EMOTION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41134cd2-452e-47d0-9e2b-d0a04a6e9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1592/1592 [15:11<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finding the underlying emotions influencing customer feedback is the aim of emotion analysis. \n",
    "#Stakeholders can better understand the underlying reasons for customer satisfaction or dissatisfaction and adjust their responses\n",
    "#by understanding the emotional states of their users.\n",
    "#We are categorizing the reviews as an emotion score and an emotion label, which indicate the various classes of the emotions, using hugging face\n",
    "#transformers pretrained models.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-emotion-analysis\",max_length=512,truncation=True)\n",
    "reviews_skip['Emotion_score']=np.nan\n",
    "reviews_skip['Emotion_class']=np.nan\n",
    "from tqdm import tqdm\n",
    "for j in tqdm(range(0,len(reviews_skip))):\n",
    "    try:\n",
    "        res=pipe(reviews_skip['content'][j])\n",
    "        reviews_skip['Emotion_score'][j]=res[0]['score']\n",
    "        reviews_skip['Emotion_class'][j]=res[0]['label']\n",
    "    except:\n",
    "        reviews_skip['Emotion_score'][j]=np.nan\n",
    "        reviews_skip['Emotion_class'][j]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1751c0-5033-4631-843b-fbb4c2047721",
   "metadata": {},
   "source": [
    "### TOPIC MODELLING - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4469b9f6-258c-4b08-83d5-de9f15238560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hemak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#To find latent topics or themes in the corpus of customer reviews, topic modeling approaches like Latent Dirichlet Allocation (LDA) or\n",
    "#Non-Negative Matrix Factorization (NMF) are used. These algorithms use the word distribution to automatically classify reviews into subjects. \n",
    "#Finding common issues, worries, or areas of interest among clients is the aim of topic modeling. Stakeholders can prioritize and address the most \n",
    "#pertinent issues influencing user experience by grouping reviews into topics.\n",
    "\n",
    "#LDA is a generative probabilistic model. \n",
    "#The LDA model generates a distribution of topics, where each topic is represented by a distribution of terms (words) from the corpus. \n",
    "#These distributions provide information about which terms are most associated with each topic.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "def preprocess_data(documents):\n",
    "    stop_words = stopwords.words('english')\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in documents]\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a96c8f-7556-4f0f-8463-b1ae8be24277",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=reviews_skip['content']\n",
    "processed_texts = preprocess_data(documents)\n",
    "id2word = corpora.Dictionary(processed_texts)\n",
    "# Create Corpus\n",
    "texts = processed_texts\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2041ad98-bd35-4db3-8f7f-90317038a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.5185896430672139\n"
     ]
    }
   ],
   "source": [
    "#The code initializes an LDA model with specific parameters such as the number of topics (num_topics), the document-term matrix (corpus) \n",
    "#representing term frequencies in each document, and a dictionary (id2word) mapping word IDs to words.\n",
    "#Coherence is a measure of how interpretable and distinct the topics discovered by the model are.\n",
    "num_topics = 2\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, random_state=42, passes=10, alpha='auto', per_word_topics=True)\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d8725a-7ce7-49fe-a738-47b4d77060e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"food\" + 0.022*\"service\" + 0.021*\"order\" + 0.017*\"app\" + 0.016*\"skip\" '\n",
      "  '+ 0.015*\"delivery\" + 0.014*\"get\" + 0.013*\"drivers\" + 0.012*\"customer\" + '\n",
      "  '0.010*\"refund\"'),\n",
      " (1,\n",
      "  '0.020*\"order\" + 0.019*\"app\" + 0.012*\"service\" + 0.011*\"customer\" + '\n",
      "  '0.010*\"skip\" + 0.010*\"great\" + 0.007*\"support\" + 0.006*\"time\" + 0.006*\"get\" '\n",
      "  '+ 0.005*\"even\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "981535e3-4672-43d6-948d-400109ccd754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2516825762566960162943617674\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2516825762566960162943617674_data = {\"mdsDat\": {\"x\": [0.06543944888171693, -0.06543944888171693], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [78.22899342720328, 21.77100657279673]}, \"tinfo\": {\"Term\": [\"app\", \"order\", \"great\", \"customer\", \"delivery\", \"support\", \"chat\", \"back\", \"works\", \"got\", \"contact\", \"skip\", \"friendly\", \"go\", \"wait\", \"door\", \"issues\", \"best\", \"service\", \"courier\", \"hours\", \"drivers\", \"issue\", \"phone\", \"restaurant\", \"tried\", \"still\", \"pizza\", \"first\", \"time\", \"make\", \"delivery\", \"company\", \"awful\", \"takes\", \"refused\", \"drivers\", \"deliver\", \"paid\", \"give\", \"arrived\", \"convenient\", \"worst\", \"cold\", \"disappointed\", \"eat\", \"customers\", \"fee\", \"taking\", \"house\", \"charges\", \"zero\", \"end\", \"receive\", \"recommend\", \"job\", \"cost\", \"refuse\", \"stops\", \"trash\", \"food\", \"delivered\", \"could\", \"worse\", \"instructions\", \"good\", \"terrible\", \"pay\", \"wrong\", \"find\", \"refund\", \"follow\", \"use\", \"get\", \"never\", \"service\", \"ever\", \"skip\", \"orders\", \"address\", \"way\", \"hour\", \"would\", \"order\", \"times\", \"tip\", \"time\", \"customer\", \"app\", \"driver\", \"even\", \"money\", \"always\", \"support\", \"great\", \"like\", \"gluten\", \"new\", \"gift\", \"responding\", \"push\", \"email\", \"open\", \"promo\", \"advertised\", \"burgers\", \"wings\", \"sub\", \"resolved\", \"buggy\", \"foods\", \"button\", \"delicious\", \"download\", \"worked\", \"rectified\", \"stuck\", \"starting\", \"discounted\", \"cad\", \"create\", \"heard\", \"salad\", \"voucher\", \"moving\", \"category\", \"payment\", \"friendly\", \"works\", \"courier\", \"found\", \"phone\", \"pizza\", \"menu\", \"deals\", \"awesome\", \"best\", \"okay\", \"great\", \"app\", \"order\", \"team\", \"back\", \"contact\", \"chat\", \"support\", \"customer\", \"issues\", \"got\", \"service\", \"skip\", \"go\", \"issue\", \"wait\", \"door\", \"hours\", \"first\", \"time\", \"restaurant\", \"tried\", \"even\", \"still\", \"get\", \"driver\", \"always\", \"ordered\", \"points\", \"food\", \"using\"], \"Freq\": [400.0, 463.0, 148.0, 263.0, 272.0, 131.0, 72.0, 62.0, 27.0, 81.0, 48.0, 332.0, 21.0, 52.0, 56.0, 59.0, 42.0, 24.0, 446.0, 21.0, 59.0, 230.0, 45.0, 21.0, 88.0, 39.0, 54.0, 18.0, 32.0, 191.0, 72.0345458581715, 269.3327773427016, 51.83287497752346, 26.840425582827766, 27.76188372924717, 24.95544046013367, 226.18285886291977, 42.39643246877413, 23.037625425710857, 73.52981661625365, 19.271455223144127, 26.60968616329228, 75.21045571656144, 100.78847634599008, 17.39277755697148, 16.453721422488584, 14.618502520971424, 19.138832704404763, 12.752495078437994, 23.669139280582318, 12.739837146042403, 20.925709120465157, 17.282953246620288, 31.798074117441868, 31.792686455651868, 19.048840315646338, 13.60313824896858, 11.780326333622952, 16.3060493001978, 19.919949431086806, 418.2119898934805, 90.21160718559968, 62.02860496427262, 31.686467576764706, 53.73843564204978, 156.1878154481633, 63.161898597528605, 31.519345585703903, 92.60477787314876, 34.94723413261744, 175.20138604022088, 31.44254989552007, 159.75535710517104, 237.73264550971618, 155.44383466556076, 386.83244491279436, 54.825583849647394, 282.43799628805493, 115.436836346273, 45.56512736715481, 65.35836841622762, 57.407941359109024, 85.46101694113231, 366.48938785670066, 92.10442592756561, 50.88394180451567, 160.78582219424044, 212.079754818272, 306.9297577765084, 117.65479125883266, 111.24421939751392, 74.5872077230294, 79.72711901231261, 97.84973832463395, 98.87141107742853, 68.27183154360466, 5.841525947726257, 8.372588645260327, 5.794369659076929, 5.127103186264503, 5.763562860192279, 8.940016506730958, 7.022956451556198, 4.456951978874726, 4.412880816937914, 5.665924286544951, 3.7742456198585237, 4.371507965031656, 4.34067921707957, 3.7144375466754833, 3.701380131973435, 3.081208091347564, 11.08192565925211, 3.0771480839774834, 3.065102036694451, 3.0640900991697877, 3.0466807830060376, 3.627865008702795, 2.4046893739786497, 2.398163374629452, 2.393303509035652, 2.393102546794838, 2.9908888190582053, 2.990981176173132, 2.390700834227499, 2.3902866524744466, 8.597170933163596, 15.516998693685135, 18.98119207199035, 14.615991185739174, 6.532545302244367, 14.020901228136594, 12.57508366008112, 7.471076593816154, 6.962561169556029, 8.748820745768171, 14.902440768424968, 5.3546956319876164, 49.223861156341364, 93.3524372927219, 96.82775571263629, 7.5904892124660055, 22.703175822071586, 18.937883426357335, 24.229426175326868, 34.132227408093975, 51.52688273500758, 16.48321716603737, 23.5908964477923, 59.3030507960836, 49.898469449684846, 17.927550178986237, 16.50083685118512, 18.224182988751213, 18.540397787303313, 18.24395539282467, 13.373977265045324, 30.33149040405105, 21.11187413399139, 14.450610378609795, 24.384741197140492, 16.20029311667694, 27.350832130840782, 21.785262401248794, 17.714607738092017, 16.621153087286572, 16.20128053965875, 17.190334023623876, 15.319694250707267], \"Total\": [400.0, 463.0, 148.0, 263.0, 272.0, 131.0, 72.0, 62.0, 27.0, 81.0, 48.0, 332.0, 21.0, 52.0, 56.0, 59.0, 42.0, 24.0, 446.0, 21.0, 59.0, 230.0, 45.0, 21.0, 88.0, 39.0, 54.0, 18.0, 32.0, 191.0, 72.62720278302771, 272.31128489235596, 52.66769414619338, 27.273244944661982, 28.212666941882333, 25.383045315029612, 230.26784142121448, 43.21333371577934, 23.4837863999125, 75.09952127713494, 19.70428659282855, 27.21021421614184, 76.93416415626665, 103.17668197032401, 17.815768546860607, 16.871518322347946, 14.994929668873988, 19.667968671784106, 13.109870756490217, 24.34412448502213, 13.106373424168247, 21.531711969112763, 17.78580367358565, 32.75398208662907, 32.75250834263693, 19.64357438479834, 14.029842693257192, 12.15663162144332, 16.831279901364, 20.569144837604917, 435.40232391710435, 93.41172078662346, 64.39422984411253, 32.723551250073875, 55.94047013154692, 164.39062891196178, 66.07985722171071, 32.678012768706154, 98.19430572247961, 36.36529368426574, 188.84612700116125, 32.657063202506684, 175.00109497083636, 265.08347764055696, 170.38473248055683, 446.13549570887795, 58.301859191891445, 332.3364657377398, 129.19718219024224, 48.206965297723286, 70.8088611806205, 61.7591316140152, 94.8720302194484, 463.31714356933696, 103.56597809635196, 54.474723721258314, 191.11731259829148, 263.6066375532796, 400.2821950692303, 139.44005366008145, 135.62896059465442, 85.02618647618857, 97.44172675040463, 131.98196573272793, 148.0952722337699, 79.17564023555782, 6.328386973090482, 9.164888035018208, 6.345749726849421, 5.6491892514255415, 6.357302458535229, 9.898250694252402, 7.7774940040629845, 4.954238379468485, 4.969929388613912, 6.3935017643578185, 4.263498636463045, 4.98556703763126, 4.996984830431913, 4.285339758291197, 4.290120202954058, 3.576685790629729, 12.881321987284865, 3.5780440131164815, 3.5826188640454597, 3.5828970589834137, 3.5891565489563297, 4.317483093130017, 2.8837936323521465, 2.886119793065073, 2.8879593914424713, 2.8880071348344303, 3.6099077925628187, 3.610235283729013, 2.888931932124431, 2.889025692415969, 10.972608428969291, 21.628452898628705, 27.901327576657902, 21.965595623323857, 8.906011333785745, 21.242224004522978, 18.94471953449889, 10.447505969823208, 9.691534010424066, 12.8073632726344, 24.696512374567284, 7.454900430597034, 148.0952722337699, 400.2821950692303, 463.31714356933696, 12.294675403023664, 62.46106750676336, 48.73203415360422, 72.2973457348621, 131.98196573272793, 263.6066375532796, 42.08145074561122, 81.0512230872412, 446.13549570887795, 332.3364657377398, 52.8945527284377, 45.85937771011544, 56.567946608707544, 59.288000894277296, 59.39893947858434, 32.83755282024773, 191.11731259829148, 88.60161794266745, 39.05760432461043, 135.62896059465442, 54.48691474500961, 265.08347764055696, 139.44005366008145, 97.44172675040463, 83.65918959557541, 76.24722186813054, 435.40232391710435, 71.84658697536248], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.4983, -4.1795, -5.8274, -6.4856, -6.4518, -6.5584, -4.3541, -6.0284, -6.6383, -5.4778, -6.8168, -6.4942, -5.4552, -5.1625, -6.9194, -6.9749, -7.0932, -6.8238, -7.2297, -6.6113, -7.2307, -6.7345, -6.9258, -6.3161, -6.3162, -6.8285, -7.1652, -7.309, -6.9839, -6.7838, -3.7395, -5.2733, -5.6479, -6.3196, -5.7913, -4.7244, -5.6298, -6.3249, -5.2471, -6.2216, -4.6095, -6.3273, -4.7018, -4.3043, -4.7292, -3.8175, -5.7713, -4.132, -5.0268, -5.9563, -5.5956, -5.7253, -5.3274, -3.8715, -5.2526, -5.8459, -4.6954, -4.4185, -4.0489, -5.0077, -5.0637, -5.4635, -5.3969, -5.192, -5.1817, -5.552, -6.7314, -6.3714, -6.7395, -6.8619, -6.7449, -6.3059, -6.5472, -7.0019, -7.0119, -6.7619, -7.1682, -7.0213, -7.0284, -7.1842, -7.1877, -7.3711, -6.0911, -7.3724, -7.3763, -7.3767, -7.3824, -7.2078, -7.619, -7.6217, -7.6237, -7.6238, -7.4008, -7.4008, -7.6248, -7.625, -6.345, -5.7545, -5.553, -5.8143, -6.6196, -5.8559, -5.9647, -6.4854, -6.5559, -6.3275, -5.7949, -6.8184, -4.6, -3.96, -3.9235, -6.4695, -5.3739, -5.5552, -5.3088, -4.9662, -4.5543, -5.6941, -5.3356, -4.4138, -4.5864, -5.6101, -5.693, -5.5937, -5.5765, -5.5926, -5.9031, -5.0842, -5.4466, -5.8257, -5.3025, -5.7114, -5.1877, -5.4152, -5.622, -5.6857, -5.7113, -5.6521, -5.7673], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.2373, 0.2345, 0.2296, 0.2295, 0.2294, 0.2285, 0.2276, 0.2264, 0.2263, 0.2244, 0.2233, 0.2232, 0.2229, 0.2221, 0.2215, 0.2205, 0.2201, 0.2183, 0.2179, 0.2174, 0.2172, 0.217, 0.2168, 0.2159, 0.2158, 0.2148, 0.2146, 0.2141, 0.2138, 0.2135, 0.2052, 0.2107, 0.2081, 0.2133, 0.2054, 0.1943, 0.2004, 0.2094, 0.1869, 0.2058, 0.1705, 0.2076, 0.1544, 0.1366, 0.1538, 0.1029, 0.1841, 0.0828, 0.1329, 0.1892, 0.1654, 0.1725, 0.1411, 0.0111, 0.1282, 0.1773, 0.0727, 0.028, -0.02, 0.0756, 0.0473, 0.1145, 0.0449, -0.0537, -0.1585, 0.0974, 1.4445, 1.4342, 1.4337, 1.4276, 1.4265, 1.4228, 1.4225, 1.4188, 1.4057, 1.4038, 1.4027, 1.3932, 1.3838, 1.3816, 1.377, 1.3755, 1.3741, 1.3738, 1.3686, 1.3682, 1.3607, 1.3506, 1.3429, 1.3394, 1.3367, 1.3366, 1.3365, 1.3364, 1.3353, 1.3351, 1.2806, 1.1925, 1.1394, 1.1172, 1.2147, 1.1091, 1.1148, 1.1893, 1.1939, 1.1435, 1.0195, 1.1937, 0.4231, 0.0688, -0.0409, 1.0423, 0.5126, 0.5794, 0.4314, 0.1722, -0.1078, 0.5873, 0.2904, -0.4934, -0.3716, 0.4426, 0.5024, 0.3919, 0.3621, 0.3441, 0.6263, -0.3161, 0.0903, 0.5303, -0.1914, 0.3117, -0.7467, -0.3318, -0.1803, -0.0915, -0.0243, -1.7073, -0.0208]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.9542189539604246, 0.06223167091046247, 0.20121010215778837, 0.8048404086311535, 0.8210035132579154, 0.18472579048303098, 0.7669589199362296, 0.23233608975266887, 0.9642571889364989, 0.3123203359544609, 0.702720755897537, 0.9899812088654503, 0.6403989172242174, 0.368229377403925, 0.40491547342118234, 0.6073732101317735, 0.2333537260529269, 0.9334149042117076, 0.15640880957830514, 0.9384528574698309, 0.8387653195199473, 0.6929719288872589, 0.6922749095829207, 0.991883839966585, 0.6639247888301684, 0.3319623944150842, 0.9789033536574662, 0.01938422482490032, 0.9873225103734367, 0.01898697135333532, 0.6156114867981805, 0.38988727497218095, 0.9922744372950532, 0.03675090508500197, 0.9978729131958454, 0.9628191865962439, 0.031058683438588513, 0.31868018150016153, 0.6828861032146318, 0.6925305133882248, 0.804228611114358, 0.1972636215940878, 1.0003381363726258, 0.3095485190242583, 0.7222798777232693, 0.15526356704491956, 0.8539496187470575, 0.9719222376186104, 0.023141005657585964, 0.9634765235251718, 0.032115884117505725, 0.9878400746643133, 0.011016803806665204, 0.9542108697295376, 0.6935309023373887, 0.691539592861487, 0.32046956742361593, 0.279482308304251, 0.838446924912753, 0.8462417856468507, 0.15777389223924335, 0.9814657513838088, 0.01737107524573113, 0.9483438119974338, 0.10102795240179843, 0.9092515716161859, 0.9558184893970983, 0.05622461702335873, 0.8184092800927568, 0.17695335785789337, 0.9433661423896639, 0.05145633503943621, 0.966037739690811, 0.05084409156267426, 0.9624561347938059, 0.02749874670839445, 0.5786058450824796, 0.39588820979327555, 0.949258658311336, 0.030621247042301163, 0.9600316237163273, 0.03904434833296068, 0.2330937019693359, 0.9323748078773436, 0.2245674213789536, 0.7859859748263376, 0.2774123525210818, 0.7397662733895515, 0.8978303820305198, 0.101854707205143, 0.15758579254534932, 0.9455147552720959, 0.9853591439940417, 0.02663132821605518, 0.9481088981304642, 0.6616938454833167, 0.34029969196284854, 0.9489592018261861, 0.048664574452624926, 0.7032589741261134, 0.2961090417373109, 0.6684885918824437, 0.3308680909317146, 0.6925190647476223, 0.9229404382859684, 0.06476775005515568, 0.6902480138518654, 0.30303571339838, 0.9858641667218775, 0.04107767361341157, 0.965311873014585, 0.03575229159313278, 0.6323679353722089, 0.3706984448733639, 0.6178494215224175, 0.38021502862918005, 0.9672374094352, 0.05090723207553684, 0.8588500174762232, 0.13893162047409494, 0.9913640790365909, 0.013768945542174874, 0.2871498718129726, 0.6700163675636027, 0.8820811929628717, 0.11761082572838288, 0.6922973773664726, 0.9097059210847285, 0.08803605687916727, 0.10911208038538937, 0.872896643083115, 0.2682799077760221, 0.6706997694400554, 0.1285761196958297, 0.9000328378708078, 0.7899556601346155, 0.2093598334236549, 0.800868384261082, 0.20320541093191632, 0.890112292315037, 0.10836149645574364, 0.9793991313123891, 0.9792517135755744, 0.0306016160492367, 0.18227206529303527, 0.8202242938186588, 0.32953235021481425, 0.6590647004296285, 0.31671094359955154, 0.6862070444656949, 0.7869139167295816, 0.2098437111278884, 0.8073894902952045, 0.15729942165287628, 0.9437965299172576, 0.9769804451674026, 0.03053063891148133, 0.9770244057412445, 0.03053201267941389, 0.27910374859715703, 0.8373112457914711, 0.9266803761293124, 0.074134430090345, 0.9871155410214919, 0.9849094027026455, 0.2001206795565888, 0.8004827182263552, 0.17701655148969483, 0.8850827574484742, 0.7561938659331766, 0.23701598782980163, 0.27701538583899943, 0.8310461575169984, 0.867449471567117, 0.1322468186626871, 0.8485376390279653, 0.15044993599786619, 0.2316164252249652, 0.9264657008998608, 0.6974151533048653, 0.29364848560204854, 0.9506110107944534, 0.05941318817465334, 0.2786169915856092, 0.8358509747568277, 0.20057898980235545, 0.8023159592094218, 0.7425256886873194, 0.2576109532180496, 0.9924620050163842, 0.9916192341990996, 0.4066801144477825, 0.6506881831164519, 0.9533918904912704, 0.045399613832917636, 0.8424145244151957, 0.15697165051214826, 0.8883226102920437, 0.10621248601317915, 0.9362140184677553, 0.07342855046805924, 0.9723301652986373, 0.048616508264931864, 0.6400802207995986, 0.3584449236477752, 0.9142799936575468, 0.085713749405395, 0.7933571015634514, 0.2087781846219609, 0.2769902572574438, 0.8309707717723315, 0.6717585183505783, 0.3182014034292213, 0.9179642055560935, 0.07061263119662257, 0.9381966176300596, 0.2791254213602865, 0.8373762640808595, 0.3225652964101017, 0.6809711813102146, 0.9778889752965843, 0.030559030478018258, 0.9748594895716548, 0.025996253055244126, 0.8959437233859819, 0.09486462953498632, 0.9471017623245899, 0.06110333950481225, 0.9753056343185575, 0.04644312544374083], \"Term\": [\"address\", \"address\", \"advertised\", \"advertised\", \"always\", \"always\", \"app\", \"app\", \"arrived\", \"awesome\", \"awesome\", \"awful\", \"back\", \"back\", \"best\", \"best\", \"buggy\", \"buggy\", \"burgers\", \"burgers\", \"button\", \"cad\", \"category\", \"charges\", \"chat\", \"chat\", \"cold\", \"cold\", \"company\", \"company\", \"contact\", \"contact\", \"convenient\", \"convenient\", \"cost\", \"could\", \"could\", \"courier\", \"courier\", \"create\", \"customer\", \"customer\", \"customers\", \"deals\", \"deals\", \"delicious\", \"delicious\", \"deliver\", \"deliver\", \"delivered\", \"delivered\", \"delivery\", \"delivery\", \"disappointed\", \"discounted\", \"door\", \"door\", \"download\", \"download\", \"driver\", \"driver\", \"drivers\", \"drivers\", \"eat\", \"email\", \"email\", \"end\", \"end\", \"even\", \"even\", \"ever\", \"ever\", \"fee\", \"fee\", \"find\", \"find\", \"first\", \"first\", \"follow\", \"follow\", \"food\", \"food\", \"foods\", \"foods\", \"found\", \"found\", \"friendly\", \"friendly\", \"get\", \"get\", \"gift\", \"gift\", \"give\", \"give\", \"gluten\", \"go\", \"go\", \"good\", \"good\", \"got\", \"got\", \"great\", \"great\", \"heard\", \"hour\", \"hour\", \"hours\", \"hours\", \"house\", \"house\", \"instructions\", \"instructions\", \"issue\", \"issue\", \"issues\", \"issues\", \"job\", \"job\", \"like\", \"like\", \"make\", \"make\", \"menu\", \"menu\", \"money\", \"money\", \"moving\", \"never\", \"never\", \"new\", \"new\", \"okay\", \"okay\", \"open\", \"open\", \"order\", \"order\", \"ordered\", \"ordered\", \"orders\", \"orders\", \"paid\", \"pay\", \"pay\", \"payment\", \"payment\", \"phone\", \"phone\", \"pizza\", \"pizza\", \"points\", \"points\", \"promo\", \"push\", \"push\", \"receive\", \"receive\", \"recommend\", \"recommend\", \"rectified\", \"rectified\", \"refund\", \"refund\", \"refuse\", \"refused\", \"resolved\", \"resolved\", \"responding\", \"responding\", \"restaurant\", \"restaurant\", \"salad\", \"salad\", \"service\", \"service\", \"skip\", \"skip\", \"starting\", \"starting\", \"still\", \"still\", \"stops\", \"stops\", \"stuck\", \"stuck\", \"sub\", \"sub\", \"support\", \"support\", \"takes\", \"taking\", \"team\", \"team\", \"terrible\", \"terrible\", \"time\", \"time\", \"times\", \"times\", \"tip\", \"tip\", \"trash\", \"trash\", \"tried\", \"tried\", \"use\", \"use\", \"using\", \"using\", \"voucher\", \"voucher\", \"wait\", \"wait\", \"way\", \"way\", \"wings\", \"worked\", \"worked\", \"works\", \"works\", \"worse\", \"worse\", \"worst\", \"worst\", \"would\", \"would\", \"wrong\", \"wrong\", \"zero\", \"zero\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2516825762566960162943617674\", ldavis_el2516825762566960162943617674_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2516825762566960162943617674\", ldavis_el2516825762566960162943617674_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2516825762566960162943617674\", ldavis_el2516825762566960162943617674_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyLDAvis calculates the similarity (or dissimilarity) between topics based on their distributions of terms. \n",
    "#After calculating the distances between topics, MDS is applied to reduce the dimensionality of the topic space while preserving the pairwise \n",
    "#distances as much as possible. MDS techniques like Principal Component Analysis (PCA) are commonly used for this purpose.\n",
    "#By using pyLDAvis we are visualizing the topics as 1 and 2 with the word’s relevance to their\n",
    "#respective topics.\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis\n",
    "\n",
    "# Convert corpus to a bag-of-words representation\n",
    "dictionary = Dictionary.from_corpus(corpus, lda_model.id2word)\n",
    "#corpus_bow = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "# Visualize the LDA model\n",
    "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8816791-3c5d-4a77-b7cc-bf2bb6827728",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = [lda_model[doc] for doc in corpus]\n",
    "\n",
    "# Function to get the dominant topic for each document\n",
    "def get_dom_topic(topic_dist):\n",
    "    if not topic_dist:\n",
    "        return None\n",
    "    return max(topic_dist[0])[0]\n",
    "# Assign dominant topics to each document\n",
    "documents_topics = [get_dom_topic(doc) for doc in topic_distribution]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7175148-7170-41df-8f64-c5aa5261c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_skip['Topics']=documents_topics\n",
    "reviews_skip['Topics_LDA']=reviews_skip['Topics'].apply(lambda x: 'Food Based' if x == 0 else 'App Based')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4f6e9-9d26-4788-a579-bf42f0404c75",
   "metadata": {},
   "source": [
    "### TOPIC MODELLING - NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1081efd-f36c-4a10-8965-f44e2662daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Experience\n",
      "['great service', 'far good', 'service always', 'service time', 'service everytime', 'great food', 'great service quick', 'service quick', 'always great service', 'great service great', 'always great', 'service great', 'easy convenient', 'food great', 'food great service', 'please keep', 'go beyond', 'quick easy', 'points last']\n",
      "Customer Experience\n",
      "['customer service', 'poor customer', 'poor customer service', 'worst customer service', 'worst customer', 'horrible customer service', 'horrible customer', 'worst customer service ever', 'customer service ever', 'service ever', 'app customer', 'app customer service', 'terrible customer service', 'terrible customer', 'every time', 'customer service sucks', 'service sucks', 'good customer', 'good customer service']\n",
      "App Experience\n",
      "['easy use', 'good service', 'convenient easy', 'convenient easy use', 'quick easy', 'good easy', 'highly recommended', 'needs work', 'great app', 'lots options', 'app easy', 'good selection', 'use good', 'would love', 'food delivery', 'food delivery app', 'delivery app', 'app help', 'use get']\n",
      "Overall Delivery Experience\n",
      "['missing items', 'customer support', 'uber eats', 'cold food', 'door dash', 'use app', 'good app', 'order missing', 'food delivery', 'order missing items', 'credit missing', 'credit missing items', 'delivery drivers', 'worst customer', 'refund missing', 'use uber', 'worst customer support', 'wrong order', 'refund missing items']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x257d48e5290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NMF is a dimensionality reduction technique used for feature extraction and topic modeling. \n",
    "#It factorizes the TF-IDF matrix into two matrices: W (document-topic matrix) and H (topic-term matrix) to approximate the original TF-IDF matrix.\n",
    "#Each row of the W matrix represents a document's distribution over topics, and each column of the H matrix represents a topic's distribution over terms.\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "content=reviews_skip['content']\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=list(stopwords.words('english'))\n",
    "stop_words.append(\"skip\")\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stop_words,ngram_range=(2,4))\n",
    "tfidf = vectorizer.fit_transform(content)\n",
    "\n",
    "# Apply NMF\n",
    "num_topics = 4\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_output = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "# Visualization - Topic-Word Distribution\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    if topic_idx in range (0,2):\n",
    "        temp=\"Customer Experience\"\n",
    "    elif topic_idx==2:\n",
    "        temp=\"App Experience\"\n",
    "    else:\n",
    "        temp=\"Overall Delivery Experience\"\n",
    "    #print(f\"Word Cloud for Topic :{temp}\")\n",
    "    top_words_idx = topic.argsort()[:-20:-1]\n",
    "    #print(top_words_idx)\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(temp)\n",
    "    print(top_words)\n",
    "\n",
    "import numpy as np\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "nmf_components = nmf_model.components_\n",
    "num_top_words = 10\n",
    "\n",
    "top_words = []\n",
    "word_weights = []\n",
    "topic_names = []\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_components):\n",
    "    top_feature_idxs = topic.argsort()[-num_top_words:][::-1]\n",
    "    top_words.append([feature_names[i] for i in top_feature_idxs])\n",
    "    word_weights.append(topic[top_feature_idxs])\n",
    "    topic_names.append(f\"Topic {topic_idx + 1}\")\n",
    "Topic=[]\n",
    "Word=[]\n",
    "Weight=[]\n",
    "# Display top words, their weights, and topic names\n",
    "for topic_idx, (topic_words, weights, topic_name) in enumerate(zip(top_words, word_weights, topic_names)):\n",
    "    #print(f\"Topic {topic_idx + 1}: {topic_name}\")\n",
    "    for word, weight in zip(topic_words, weights):\n",
    "        #print(f\"{word}: {weight}\")\n",
    "        Topic.append(topic_name)\n",
    "        Word.append(word)\n",
    "        Weight.append(weight)\n",
    "nmf=pd.DataFrame()\n",
    "nmf['Topic']=Topic\n",
    "nmf['Word']=Word\n",
    "nmf['Weight']=Weight\n",
    "\n",
    "#The output of the NMF model which are the topics categorized are visualized using a topic_wizard using dash library.\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div([\n",
    "    html.H1('Topic Wizard'),\n",
    "    dcc.Dropdown(\n",
    "        id='topic-dropdown',\n",
    "        options=[\n",
    "            {'label': 'Customer Experience', 'value': 'customer_experience'},\n",
    "            {'label': 'App Experience', 'value': 'app_experience'},\n",
    "            {'label': 'Overall Delivery Experience', 'value': 'overall_delivery_experience'}\n",
    "        ],\n",
    "        value='customer_experience'\n",
    "    ),\n",
    "    dcc.Graph(id='topic-visualization')\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('topic-visualization', 'figure'),  # Output for word cloud\n",
    "    Input('topic-dropdown', 'value')\n",
    ")\n",
    "\n",
    "def update_topic_visualization(selected_topic):\n",
    "    # Generate visualization based on selected topic\n",
    "    if selected_topic == 'customer_experience':\n",
    "        data=nmf[(nmf['Topic']=='Topic 1' ) | (nmf['Topic']=='Topic 2')].sort_values('Weight',ascending =False)[0:10]\n",
    "        #fig = go.Figure(data=[go.Bar(x=df['Category'], y=df['Value'])],layout={'title': 'Bar Chart'})\n",
    "        fig = go.Figure(data=[go.Bar(x=data['Word'], y=data['Weight'])])\n",
    "        fig.update_layout(title='Customer Experience Topics', xaxis_title='Topics', yaxis_title='Weight')\n",
    "\n",
    "    elif selected_topic == 'app_experience':\n",
    "        # Similar logic for app experience, assuming you have topic data for app experience\n",
    "        data=nmf[(nmf['Topic']=='Topic 3')].sort_values('Weight',ascending =False)\n",
    "        #fig = go.Figure(data=[go.Bar(x=df['Category'], y=df['Value'])],layout={'title': 'Bar Chart'})\n",
    "        fig = go.Figure(data=[go.Bar(x=data['Word'], y=data['Weight'])])\n",
    "        fig.update_layout(title='App Experience Topics', xaxis_title='Topics', yaxis_title='Weight')\n",
    "\n",
    "    elif selected_topic == 'overall_delivery_experience':\n",
    "        data=nmf[(nmf['Topic']=='Topic 4' )].sort_values('Weight',ascending =False)\n",
    "        #fig = go.Figure(data=[go.Bar(x=df['Category'], y=df['Value'])],layout={'title': 'Bar Chart'})\n",
    "        fig = go.Figure(data=[go.Bar(x=data['Word'], y=data['Weight'])])\n",
    "        fig.update_layout(title='Overall Delivery Topics', xaxis_title='Topics', yaxis_title='Weight')\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Handle case where selected topic does not match any of the predefined options\n",
    "        fig = go.Figure()\n",
    "    fig.update_layout(xaxis=dict(\n",
    "        automargin=True,  # Automatically adjust margin to prevent overlap\n",
    "        tickmode='array',  # Set tick mode to array\n",
    "        tickvals=data['Word'],  # Set tick values to the data\n",
    "        ticktext=data['Word'].apply(lambda x: '<br>'.join(x.split())),  # Split text and join with <br> for new line\n",
    "    ))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cf95ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Topics to main dataframe\n",
    "topic_num=[]\n",
    "dominant_topics = np.argmax(nmf_output, axis=1)\n",
    "for idx, document in enumerate(content):\n",
    "    if dominant_topics[idx] in range (0,2):\n",
    "        topic_num.append(\"Customer Experience\")\n",
    "    elif dominant_topics[idx]==2:\n",
    "        topic_num.append(\"App Experince\")\n",
    "    else:\n",
    "        topic_num.append(\"Overall Delivery Experince\")\n",
    "reviews_skip['Topics_NMF']=topic_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79475aa9-5815-45cc-be92-59f14c06fbb6",
   "metadata": {},
   "source": [
    "#### COMBINED RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478bf571-e61a-4d96-a610-d4cdfe757ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_skip=reviews_skip.drop(['replyContent','repliedAt','score','thumbsUpCount','reviewCreatedVersion','at','appVersion','Topics'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfaea144-8745-4ae6-a448-9aef2caa17d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>Sentiment_score</th>\n",
       "      <th>Sentiment_class</th>\n",
       "      <th>Emotion_score</th>\n",
       "      <th>Emotion_class</th>\n",
       "      <th>Topics_LDA</th>\n",
       "      <th>Topics_NMF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a46db96-cb7e-40c0-8790-694df2f12a46</td>\n",
       "      <td>Dakota Leveille</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>Delivered my food peferectly to my described l...</td>\n",
       "      <td>0.999785</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.977111</td>\n",
       "      <td>joy</td>\n",
       "      <td>App Based</td>\n",
       "      <td>Overall Delivery Experince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9c7634f8-e224-4634-bcba-550fd6176871</td>\n",
       "      <td>Scott Davidson</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>Customer service is lacking. It's really gone ...</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.961397</td>\n",
       "      <td>disgust</td>\n",
       "      <td>App Based</td>\n",
       "      <td>Customer Experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>616c2389-7b74-467c-bfac-2486ec434631</td>\n",
       "      <td>Tanis Nicholson</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>additional fees are getting ridiculous</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.947310</td>\n",
       "      <td>disgust</td>\n",
       "      <td>App Based</td>\n",
       "      <td>Overall Delivery Experince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98a573af-b8e6-4105-a605-2863009fdf67</td>\n",
       "      <td>Colin Carroll</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>The absolute worst customer service. Drivers d...</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>disgust</td>\n",
       "      <td>App Based</td>\n",
       "      <td>Customer Experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>440de9bc-f4eb-46af-be19-36fde3a91703</td>\n",
       "      <td>JL Rae</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>extra fees keep adding up</td>\n",
       "      <td>0.995747</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.925498</td>\n",
       "      <td>others</td>\n",
       "      <td>App Based</td>\n",
       "      <td>Customer Experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId         userName  \\\n",
       "0  6a46db96-cb7e-40c0-8790-694df2f12a46  Dakota Leveille   \n",
       "1  9c7634f8-e224-4634-bcba-550fd6176871   Scott Davidson   \n",
       "2  616c2389-7b74-467c-bfac-2486ec434631  Tanis Nicholson   \n",
       "3  98a573af-b8e6-4105-a605-2863009fdf67    Colin Carroll   \n",
       "4  440de9bc-f4eb-46af-be19-36fde3a91703           JL Rae   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "1  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "2  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "3  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "4  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "\n",
       "                                             content  Sentiment_score  \\\n",
       "0  Delivered my food peferectly to my described l...         0.999785   \n",
       "1  Customer service is lacking. It's really gone ...         0.999805   \n",
       "2             additional fees are getting ridiculous         0.999638   \n",
       "3  The absolute worst customer service. Drivers d...         0.999781   \n",
       "4                          extra fees keep adding up         0.995747   \n",
       "\n",
       "  Sentiment_class  Emotion_score Emotion_class Topics_LDA  \\\n",
       "0        POSITIVE       0.977111           joy  App Based   \n",
       "1        NEGATIVE       0.961397       disgust  App Based   \n",
       "2        NEGATIVE       0.947310       disgust  App Based   \n",
       "3        NEGATIVE       0.969821       disgust  App Based   \n",
       "4        POSITIVE       0.925498        others  App Based   \n",
       "\n",
       "                   Topics_NMF  \n",
       "0  Overall Delivery Experince  \n",
       "1         Customer Experience  \n",
       "2  Overall Delivery Experince  \n",
       "3         Customer Experience  \n",
       "4         Customer Experience  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_skip.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70541041-48ea-4340-a43a-16044b3123bf",
   "metadata": {},
   "source": [
    "### CHATBOT DEVELOPMENT AND INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "678c8a76-a265-4c45-85ab-4ff2d9efedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sumy.parsers.plaintext import PlaintextParser                   \n",
    "from sumy.nlp.tokenizers import Tokenizer                      \n",
    "from sumy.summarizers.text_rank import TextRankSummarizer \n",
    "\n",
    "df=reviews_skip\n",
    "def preprocess(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\") and word not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "#This function accepts a user question as input and utilizes pre-trained models to predict the emotion and sentiment associated with the question. \n",
    "def response(question):\n",
    "    emotion_p=pipe(question)[0]['label']\n",
    "    sentiment_p=summarization_pipeline(question)[0]['label']\n",
    "    filtered_df = df[(df['Sentiment_class'] == sentiment_p) & (df['Emotion_class'] == emotion_p)]\n",
    "    documents=filtered_df['content'].tolist()\n",
    "    queries = [question]\n",
    "    preprocessed_documents = [preprocess(doc) for doc in documents]\n",
    "    preprocessed_queries = [preprocess(query) for query in queries]\n",
    "    \n",
    "#It then filters a reviews based on predicted sentiment and emotion, retrieves content from the filtered DataFrame as documents,\n",
    "#tokenizes and preprocesses the documents, and trains a Word2Vec model on preprocessed documents to obtain word embeddings. \n",
    "#Cosine similarity is computed between the question and each document to select top relevant documents based on similarity scores.\n",
    "#Finally, the TextRank summarization algorithm is applied to summarize the selected documents, and the summarized text is returned.    \n",
    "    \n",
    "    model = Word2Vec(sentences=preprocessed_documents, vector_size=100, window=5, min_count=1, workers=4,epochs=100)\n",
    "    for i, query in enumerate(preprocessed_queries):\n",
    "        query_vector = np.zeros((model.vector_size,))\n",
    "        for word in query:\n",
    "            if word in model.wv:\n",
    "                query_vector += model.wv[word]\n",
    "        query_vector /= len(query)  # Normalize query vector\n",
    "        relevant_documents = []\n",
    "        for j, document in enumerate(preprocessed_documents):\n",
    "            document_vector = np.zeros((model.vector_size,))\n",
    "            for word in document:\n",
    "                if word in model.wv:\n",
    "                    document_vector += model.wv[word]\n",
    "            document_vector /= len(document)  # Normalize document vector\n",
    "            similarity = np.dot(query_vector, document_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(document_vector))\n",
    "            relevant_documents.append((documents[j], similarity))\n",
    "    relevant_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Query: {queries[i]}\")\n",
    "    top_10_documents = relevant_documents[:30]\n",
    "    doc_main=[]\n",
    "    for doc, similarity in top_10_documents:\n",
    "        if similarity > 0.3 :\n",
    "            doc_main.append(doc)\n",
    "    parser = PlaintextParser.from_string('.'.join(doc_main),Tokenizer(\"english\"))         \n",
    "    # Summarize using sumy TextRank                  \n",
    "    summarizer_4 = TextRankSummarizer()                   \n",
    "    summary =summarizer_4(parser.document,4)                   \n",
    "    text_summary=\"\"                  \n",
    "    for sentence in summary:                \n",
    "        text_summary+=str(sentence)   \n",
    "    #print(doc_main)\n",
    "\n",
    "    return text_summary        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "844bf91c-112c-48eb-a61f-0efaef038360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It sets up a Gradio interface (iface) by specifying the function to be used (chatbot_interface), the input type (text), and the output type (text).\n",
    "#It also provides a title and description for the interface.\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "#import openAI\n",
    "def chatbot_interface(input_text):\n",
    "    # Use GPT-3 to generate a response\n",
    "    response_1 = response(input_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = response_1.split()\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(' '.join(filtered_text))    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Word Cloud\")\n",
    "    plt.savefig(\"wcplot.png\")\n",
    "    return response_1.strip(),\"wcplot.png\"\n",
    "\n",
    "\n",
    "input_textbox = gr.Textbox(label=\"Enter your question here\")\n",
    "output_textbox = gr.Textbox(label=\"Chatbot Response\")\n",
    "output_image = gr.Image(label=\"Word Cloud\")\n",
    "# Define Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=input_textbox,\n",
    "    outputs=[output_textbox,output_image],\n",
    "    title=\"Review QA\",\n",
    "    description=\"Enter a Question and the chatbot will respond based upon customer reviews\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d317115",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "#### Gradio\n",
    "Abid, A. (n.d.). Interface. Gradio. \\\n",
    "https://www.gradio.app/docs/interface\n",
    "#### Hugging face- Emotion Prediction\n",
    "Finite Automata. (n.d.). BERTweet-base-emotion-analysis. \\\n",
    "https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis\n",
    "#### Hugging face- Sentiment Prediction\n",
    "Hugging Face. (n.d.). DistilBERT-base-uncased-finetuned-sst-2-english. \\\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "#### Google Play Scraper\n",
    "JoMingyu. (n.d.). Google-play-scraper. GitHub. \\\n",
    "https://github.com/JoMingyu/google-play-scraper.\n",
    "#### Dash Plotly\n",
    "Plotly Technologies Inc. (n.d.). Dash User Guide. Plotly. \\\n",
    "https://dash.plotly.com/#dash-fundamentals\n",
    "#### LDA\n",
    "Rehurek, R. (n.d.). Topic modeling for fun and profit. Gensim. \\\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
    "#### Word2Vec\n",
    "Radim Rehurek. (n.d.). Word2Vec - Gensim. \\\n",
    "https://radimrehurek.com/gensim/models/word2vec.html.\n",
    "#### NMF Model\n",
    "Scikit-learn developers. (n.d.). sklearn.decomposition.NMF. Scikit-learn: Machine Learning in Python. \\\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "#### LDAvis\n",
    "Sievert, C., & Shirley, K. E. (n.d.). LDAvis: A method for visualizing and interpreting topics. \\\n",
    "https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "http://vis.stanford.edu/files/2012-Termite-AVI.pdf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c54c7f-74dc-4763-a660-0ca0792b539d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
